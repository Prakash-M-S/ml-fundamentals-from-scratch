# Linear Regression: Bike Sharing Prediction ğŸš´â€â™‚ï¸

## Overview
Implementation of **Linear Regression from scratch** following Andrew Ng's Machine Learning Course concepts, applied to predict daily bike rental counts. This project demonstrates core ML fundamentals including gradient descent, cost functions, and feature scaling.

## Dataset
- **Source**: Daily bike sharing dataset with weather and seasonal features
- **Features**: Season, year, month, holiday, working day, weather situation, temperature, humidity, wind speed
- **Target**: Daily bike rental count
- **Size**: 731 days of data

## Key Implementations

### ğŸ”§ From Scratch Implementation
- **Cost Function**: Mean Squared Error (MSE)
- **Gradient Descent**: Custom implementation with learning rate Î± = 0.01
- **Feature Scaling**: Z-score normalization for both features and target
- **Train/Test Split**: 80/20 random split with seed=42

### ğŸ“Š Scikit-learn Comparison
Direct performance comparison between custom implementation and sklearn's LinearRegression.

## Results

| Metric | From Scratch | Scikit-learn |
|--------|-------------|--------------|
| MAE    | 651.99 | 651.96 |
| RMSE   | 859.36 | 857.62 |

*Results show excellent agreement between custom implementation and sklearn, validating the correctness of the from-scratch gradient descent algorithm.*

## Visualizations
- **Learning Curve**: Cost function decrease over iterations
- **Predicted vs Actual**: Scatter plot showing model performance
- **Regression Line**: Model fit visualization

## Key Learnings from Andrew Ng's Course
1. **Gradient Descent Mechanics**: Understanding how parameters update iteratively
2. **Feature Scaling Importance**: Normalized features improve convergence
3. **Cost Function Interpretation**: MSE provides intuitive loss measurement
4. **Overfitting Prevention**: Proper train/test split validation

## How to Run

### Prerequisites
```bash
pip install pandas numpy matplotlib scikit-learn
```

### Execution
1. Open `notebooks/scratch.ipynb` in Jupyter/VS Code
2. Run all cells sequentially
3. View results in `results/` folder

## File Structure
```
linear_regression_bikes/
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â””â”€â”€ day.csv              # Bike sharing dataset
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ scratch.ipynb        # Main implementation notebook
â””â”€â”€ results/
    â”œâ”€â”€ cost_curve.png       # Learning curve visualization
    â”œâ”€â”€ predicted_vs_actual.png  # Model performance plot
    â””â”€â”€ regression_line.png   # Regression fit visualization
```

## Andrew Ng Course Concepts Demonstrated
- âœ… **Linear Regression Theory**: Mathematical foundation
- âœ… **Gradient Descent**: Parameter optimization algorithm
- âœ… **Feature Normalization**: Preprocessing for better convergence
- âœ… **Cost Function**: MSE loss function implementation
- âœ… **Model Evaluation**: Performance metrics and visualization

## Advanced Features Implemented

- âœ… **Ridge Regularization**: L2 penalty to prevent overfitting
- âœ… **Cross-Validation**: 5-fold CV for robust model evaluation  
- âœ… **Overfitting Analysis**: Comprehensive diagnostics with multiple validation methods
- âœ… **Learning Curve Analysis**: Cost function convergence monitoring

## Next Steps

- [ ] Feature engineering and polynomial features
- [ ] Learning rate optimization (adaptive methods)
- [ ] Lasso (L1) regularization implementation

---
*This project demonstrates practical application of Andrew Ng's Machine Learning Course fundamentals. The from-scratch implementation provides deep understanding of underlying algorithms before using high-level libraries.*
